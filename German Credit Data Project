# german_credit_risk.py

# Import required libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.impute import SimpleImputer
import openml  # To fetch the dataset

# 1. Load and Explore the Dataset
def load_and_explore_data():
    # Load dataset from OpenML
    dataset = openml.datasets.get_dataset(31)  # ID for German Credit Dataset
    df, _, _, _ = dataset.get_data()

    # Display first few rows of the data
    print(df.head())

    # Check class balance
    sns.countplot(x='class', data=df)
    plt.title("Class Distribution: Good vs. Bad Credit Risk")
    plt.show()

    return df

# 2. Data Preprocessing: Handle missing values, encode categorical features, and scale numerical data
def preprocess_data(df):
    # Separate features and target variable
    X = df.drop(columns=['class'])
    y = df['class'].apply(lambda x: 1 if x == 'bad' else 0)  # Convert to binary (1 = bad credit, 0 = good credit)

    # Identify categorical & numerical columns
    categorical_features = X.select_dtypes(include=['object']).columns
    numerical_features = X.select_dtypes(exclude=['object']).columns

    # Create preprocessing pipeline
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])

    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    preprocessor = ColumnTransformer(transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

    # Split into training & test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Transform data
    X_train = preprocessor.fit_transform(X_train)
    X_test = preprocessor.transform(X_test)

    return X_train, X_test, y_train, y_test

# 3. Train and Evaluate Machine Learning Models
def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    # Define the models to train
    models = {
        "Logistic Regression": LogisticRegression(),
        "Decision Tree": DecisionTreeClassifier(),
        "Random Forest": RandomForestClassifier(n_estimators=100)
    }

    # Train models and evaluate performance
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"{name} Accuracy: {accuracy:.2f}")
        print(classification_report(y_test, y_pred))

# Main function to run the entire workflow
def main():
    # Step 1: Load and explore the dataset
    df = load_and_explore_data()

    # Step 2: Preprocess the data
    X_train, X_test, y_train, y_test = preprocess_data(df)

    # Step 3: Train models and evaluate their performance
    train_and_evaluate_models(X_train, X_test, y_train, y_test)

if __name__ == "__main__":
    main()
