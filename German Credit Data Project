# german_credit_risk.py

# Import necessary libraries
import openml  # For loading the dataset from OpenML
import pandas as pd  # For data manipulation
import seaborn as sns  # For data visualization
import matplotlib.pyplot as plt  # For data visualization
from sklearn.model_selection import train_test_split  # For splitting the data
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For preprocessing
from sklearn.pipeline import Pipeline  # For pipeline creation
from sklearn.compose import ColumnTransformer  # For transforming data columns
from sklearn.impute import SimpleImputer  # For handling missing values
from sklearn.linear_model import LogisticRegression  # For Logistic Regression model
from sklearn.tree import DecisionTreeClassifier  # For Decision Tree model
from sklearn.ensemble import RandomForestClassifier  # For Random Forest model
from sklearn.metrics import classification_report, accuracy_score  # For evaluating the model

# Step 1: Load & Explore the Dataset

# Load the German Credit dataset from OpenML
dataset = openml.datasets.get_dataset(31)  # Dataset ID for German Credit dataset
df, _, _, _ = dataset.get_data()

# Display the first few rows of the dataset
print(df.head())

# Visualize the class distribution (Good vs Bad Credit)
sns.countplot(x='class', data=df)
plt.title("Class Distribution: Good vs. Bad Credit Risk")
plt.show()

# Step 2: Data Preprocessing

# Separate features (X) and target (y)
X = df.drop(columns=['class'])
y = df['class'].apply(lambda x: 1 if x == 'bad' else 0)  # Convert to binary: 1 = bad, 0 = good

# Identify categorical and numerical columns
categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(exclude=['object']).columns

# Create preprocessing pipeline for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing categorical values
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical features
])

# Create preprocessing pipeline for numerical data
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing numerical values
    ('scaler', StandardScaler())  # Scale numerical features
])

# Combine transformations using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, numerical_features),
    ('cat', categorical_transformer, categorical_features)
])

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply transformations to the data
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

# Step 3: Train Machine Learning Models

# Define a dictionary of models to evaluate
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100)
}

# Loop through each model, train, predict, and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Make predictions
    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
    print(f"{name} Accuracy: {accuracy:.2f}")
    print(classification_report(y_test, y_pred))  # Print classification report

# Step 4: Next Steps
# After evaluating the models, consider hyperparameter tuning and feature selection to improve performance.
